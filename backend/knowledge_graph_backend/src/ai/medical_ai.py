"""
åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹æ¨¡å—
"""
from typing import Dict, Any, List, Optional, Tuple
import json
import re
import requests
from collections import defaultdict

from src.config.ai_config import AIConfig, ModelType

class MedicalKnowledgeGraphAI:
    """åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹"""
    
    def __init__(self, knowledge_graph_data: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–åŒ»ç–—AIåŠ©æ‰‹
        
        Args:
            knowledge_graph_data: çŸ¥è¯†å›¾è°±æ•°æ®ï¼ŒåŒ…å«nodeså’Œlinks
        """
        self.knowledge_graph_data = knowledge_graph_data or {"nodes": [], "links": []}
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
        self.sources_per_page = AIConfig.SOURCES_PAGE_SIZE
        
        # æ„å»ºå®ä½“ç´¢å¼•ç”¨äºå¿«é€ŸæŸ¥æ‰¾
        self.entity_index = self._build_entity_index()
        
        # åˆå§‹åŒ–LLM
        self.llm = self._init_llm()
    
    def _init_llm(self):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            return self._init_ollama()
        else:
            return self._init_openai()
    
    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaæ¨¡å‹"""
        try:
            # æ£€æŸ¥OllamaæœåŠ¡å¯ç”¨æ€§
            response = requests.get(f"{AIConfig.OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"[ä¿¡æ¯] OllamaæœåŠ¡å¯ç”¨: {AIConfig.OLLAMA_BASE_URL}")
                return {"type": "ollama", "available": True}
            else:
                print(f"[è­¦å‘Š] OllamaæœåŠ¡ä¸å¯ç”¨ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {"type": "ollama", "available": False}
        except Exception as e:
            print(f"[é”™è¯¯] æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {str(e)}")
            return {"type": "ollama", "available": False}
    
    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIæ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ OpenAIçš„åˆå§‹åŒ–é€»è¾‘
        return {"type": "openai", "available": True}
    
    def _build_entity_index(self) -> Dict[str, Any]:
        """æ„å»ºå®ä½“ç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢"""
        index = {
            "entities": {},  # å®ä½“IDåˆ°å®ä½“ä¿¡æ¯çš„æ˜ å°„
            "relationships": defaultdict(list),  # å®ä½“é—´å…³ç³»
            "search_index": {}  # æœç´¢ç´¢å¼•ï¼ˆå®ä½“åç§°åˆ°IDçš„æ˜ å°„ï¼‰
        }
        
        # æ„å»ºèŠ‚ç‚¹ç´¢å¼•
        for node in self.knowledge_graph_data.get("nodes", []):
            entity_id = node.get("id")
            entity_label = node.get("label", "")
            
            index["entities"][entity_id] = {
                "id": entity_id,
                "label": entity_label,
                "type": node.get("type", "entity"),
                "connections": node.get("connections", 0)
            }
            
            # æ„å»ºæœç´¢ç´¢å¼•
            if entity_label:
                # æ”¯æŒæ¨¡ç³Šæœç´¢
                label_lower = entity_label.lower()
                index["search_index"][label_lower] = entity_id
                
                # æ·»åŠ éƒ¨åˆ†åŒ¹é…
                words = label_lower.split()
                for word in words:
                    if len(word) > 1:  # é¿å…å•å­—ç¬¦ç´¢å¼•
                        if word not in index["search_index"]:
                            index["search_index"][word] = []
                        if isinstance(index["search_index"][word], list):
                            index["search_index"][word].append(entity_id)
                        else:
                            index["search_index"][word] = [index["search_index"][word], entity_id]
        
        # æ„å»ºå…³ç³»ç´¢å¼•
        for link in self.knowledge_graph_data.get("links", []):
            source = link.get("source")
            target = link.get("target")
            relation = link.get("label", "ç›¸å…³")
            
            if source and target:
                index["relationships"][source].append({
                    "target": target,
                    "relation": relation,
                    "direction": "outgoing"
                })
                index["relationships"][target].append({
                    "target": source,
                    "relation": relation,
                    "direction": "incoming"
                })
        
        return index
    
    def search_entities(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢å®ä½“ - æ”¹è¿›çš„åŒ»ç–—æœ¯è¯­åŒ¹é…ç­–ç•¥"""
        query_lower = query.lower().strip()
        if not query_lower:
            return []
        
        results = []
        scored_results = []
        
        # å®šä¹‰åŒ»ç–—æœ¯è¯­æ˜ å°„
        medical_terms_mapping = {
            'æ„Ÿå†’': ['æ„Ÿå†’', 'æ™®é€šæ„Ÿå†’', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“', 'æµæ„Ÿ'],
            'å‘çƒ§': ['å‘çƒ­', 'å‘çƒ§', 'ä½“æ¸©å‡é«˜', 'é«˜çƒ­'],
            'å’³å—½': ['å’³å—½', 'å’³ç—°', 'å¹²å’³'],
            'å¤´ç—›': ['å¤´ç—›', 'å¤´ç–¼', 'åå¤´ç—›'],
            'åƒä»€ä¹ˆ': ['é¥®é£Ÿ', 'é£Ÿç‰©', 'è¥å…»', 'æ²»ç–—', 'è¯ç‰©'],
            'æ²»ç–—': ['æ²»ç–—', 'ç–—æ³•', 'åŒ»æ²»', 'è¯ç‰©'],
            'ç—‡çŠ¶': ['ç—‡çŠ¶', 'è¡¨ç°', 'ä½“å¾'],
        }
        
        # æ‰©å±•æŸ¥è¯¢è¯
        expanded_queries = [query_lower]
        for term, synonyms in medical_terms_mapping.items():
            if term in query_lower:
                expanded_queries.extend(synonyms)
        
        # ç§»é™¤é‡å¤
        expanded_queries = list(set(expanded_queries))
        
        # å¯¹æ¯ä¸ªå®ä½“è¿›è¡Œè¯„åˆ†åŒ¹é…
        for entity_id, entity_info in self.entity_index["entities"].items():
            entity_label = entity_info.get("label", "").lower()
            score = 0
            match_type = "none"
            
            # ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
            for q in expanded_queries:
                if q == entity_label:
                    score = 100
                    match_type = "exact"
                    break
            
            # åŒ…å«åŒ¹é…
            if score < 100:
                for q in expanded_queries:
                    if q in entity_label or entity_label in q:
                        score = max(score, 80)
                        match_type = "contains"
            
            # è¯è¯­åŒ¹é…
            if score < 80:
                entity_words = set(entity_label.split())
                for q in expanded_queries:
                    query_words = set(q.split())
                    intersection = entity_words & query_words
                    if intersection:
                        score = max(score, 60 * len(intersection) / len(query_words))
                        match_type = "partial"
            
            # ç–¾ç—…ç‰¹æ®ŠåŒ¹é…
            if score < 60:
                for q in expanded_queries:
                    if ('ç–¾ç—…' in entity_label or 'ç—…' in entity_label) and any(w in entity_label for w in q.split()):
                        score = max(score, 50)
                        match_type = "disease_related"
            
            # å¦‚æœæœ‰åŒ¹é…ï¼Œæ·»åŠ åˆ°ç»“æœ
            if score > 0:
                result_entity = {
                    **entity_info,
                    "match_type": match_type,
                    "match_score": score
                }
                scored_results.append(result_entity)
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œä¼˜å…ˆæ˜¾ç¤ºè¿æ¥æ•°å¤šçš„
        scored_results.sort(key=lambda x: (x["match_score"], x.get("connections", 0)), reverse=True)
        
        return scored_results[:limit]
    
    def get_entity_context(self, entity_id: str, depth: int = 1) -> Dict[str, Any]:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        entity = self.entity_index["entities"].get(entity_id)
        if not entity:
            return {}
        
        context = {
            "entity": entity,
            "relationships": [],
            "neighbors": []
        }
        
        # è·å–ç›´æ¥å…³ç³»
        relationships = self.entity_index["relationships"].get(entity_id, [])
        for rel in relationships:
            neighbor_id = rel["target"]
            neighbor = self.entity_index["entities"].get(neighbor_id)
            if neighbor:
                context["relationships"].append({
                    "relation": rel["relation"],
                    "direction": rel["direction"],
                    "neighbor": neighbor
                })
                context["neighbors"].append(neighbor)
        
        return context
    
    def _call_ollama(self, prompt: str, context: str = "") -> str:
        """è°ƒç”¨Ollamaæ¨¡å‹"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            if context:
                full_prompt += f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼š\n{context}\n\n"
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\nè¯·åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def ask(self, question: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›å›ç­”"""
        if not question.strip():
            return {
                "answer": "è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜ã€‚",
                "related_entities": [],
                "suggested_focus": None,
                "sources": [],
                "medical_disclaimer": True
            }
        
        # æœç´¢ç›¸å…³å®ä½“
        related_entities = self.search_entities(question, limit=8)
        
        # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        if related_entities:
            context_info.append("=== çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ ===")
            for entity in related_entities:
                entity_context = self.get_entity_context(entity["id"])
                context_info.append(f"\nå®ä½“: {entity['label']}")
                context_info.append(f"ID: {entity['id']}")
                context_info.append(f"ç±»å‹: {entity.get('type', 'æœªçŸ¥')}")
                context_info.append(f"è¿æ¥æ•°: {entity.get('connections', 0)}")
                
                # æ·»åŠ å…³ç³»ä¿¡æ¯
                relationships = entity_context.get("relationships", [])
                if relationships:
                    context_info.append("å…³ç³»:")
                    for rel in relationships[:5]:  # é™åˆ¶å…³ç³»æ•°é‡
                        direction_symbol = "â†’" if rel['direction'] == 'outgoing' else "â†"
                        context_info.append(f"  {direction_symbol} {rel['relation']}: {rel['neighbor']['label']} (ID: {rel['neighbor']['id']})")
                else:
                    context_info.append("å…³ç³»: æ— ç›´æ¥å…³ç³»")
                    
            context_info.append("\n=== ä¸Šä¸‹æ–‡ç»“æŸ ===")
        
        context_text = "\n".join(context_info) if context_info else "æœªæ‰¾åˆ°ç›¸å…³å®ä½“"
        
        # è°ƒç”¨AIæ¨¡å‹
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            answer = self._call_ollama_strict(question, context_text, related_entities)
        else:
            answer = "OpenAIæ¨¡å‹æš‚æœªå®ç°"
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å®ä½“ï¼Œæä¾›æ ‡å‡†å›ç­”
        if not related_entities:
            answer = self._generate_no_knowledge_response(question)
        
        # éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨
        validated_answer = self._validate_entity_references(answer, related_entities)
        
        # å»ºè®®èšç„¦çš„èŠ‚ç‚¹ï¼ˆé€‰æ‹©æœ€ç›¸å…³çš„å®ä½“ï¼‰
        suggested_focus = related_entities[0]["id"] if related_entities else None
        
        # ä¿å­˜åˆ°èŠå¤©å†å²
        self.chat_history.append({
            "question": question,
            "answer": validated_answer,
            "related_entities": related_entities,
            "context_used": context_text,
            "timestamp": self._get_timestamp()
        })
        
        # æ›´æ–°å½“å‰æ¥æº
        self.current_sources = related_entities
        self.current_page = 0
        
        return {
            "answer": validated_answer,
            "related_entities": related_entities,
            "suggested_focus": suggested_focus,
            "sources": self._get_paginated_sources(),
            "medical_disclaimer": True,
            "context_used": context_text,
            "knowledge_graph_coverage": len(related_entities) > 0
        }

    def _call_ollama_strict(self, prompt: str, context: str = "", entities: List[Dict] = None) -> str:
        """ä¸¥æ ¼è°ƒç”¨Ollamaæ¨¡å‹ï¼Œå¼ºåŒ–çŸ¥è¯†å›¾è°±çº¦æŸ"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºä¸¥æ ¼çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            
            if context and entities:
                full_prompt += f"{context}\n\n"
                full_prompt += "ã€å¯å¼•ç”¨çš„å®ä½“IDåˆ—è¡¨ã€‘ï¼š\n"
                for entity in entities:
                    full_prompt += f"- {entity['label']} (ID: {entity['id']})\n"
                full_prompt += "\n"
            else:
                full_prompt += "çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼šæ— ç›¸å…³å®ä½“\n\n"
            
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\n"
            full_prompt += "è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°çº¦æŸå›ç­”ï¼Œåªèƒ½ä½¿ç”¨ä¸Šè¿°ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å’Œå®ä½“IDã€‚å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # é™ä½æ¸©åº¦ï¼Œå‡å°‘åˆ›é€ æ€§
                        "top_p": 0.8,
                        "top_k": 10
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"

    def _validate_entity_references(self, answer: str, valid_entities: List[Dict]) -> str:
        """éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨ï¼Œç§»é™¤æ— æ•ˆçš„å®ä½“ID"""
        if not valid_entities:
            return answer
        
        # æå–æœ‰æ•ˆçš„å®ä½“ID
        valid_ids = set(entity['id'] for entity in valid_entities)
        valid_labels = set(entity['label'] for entity in valid_entities)
        
        # æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«æ— æ•ˆçš„å®ä½“IDæ¨¡å¼
        import re
        
        # æŸ¥æ‰¾å¯èƒ½çš„å®ä½“IDæ¨¡å¼ï¼ˆå¦‚ D123, F456, B001 ç­‰ï¼‰
        id_pattern = r'\b[A-Z]\d+\b'
        found_ids = re.findall(id_pattern, answer)
        
        # ç§»é™¤æ— æ•ˆçš„å®ä½“ID
        validated_answer = answer
        for found_id in found_ids:
            if found_id not in valid_ids:
                # ç§»é™¤æ— æ•ˆçš„å®ä½“IDå¼•ç”¨
                validated_answer = re.sub(rf'\([^)]*{re.escape(found_id)}[^)]*\)', '', validated_answer)
                validated_answer = re.sub(rf'\b{re.escape(found_id)}\b', '', validated_answer)
        
        # å¦‚æœå›ç­”ä¸­æ²¡æœ‰å¼•ç”¨ä»»ä½•æœ‰æ•ˆå®ä½“ä¸”åŸæœ¬æœ‰ç›¸å…³å®ä½“ï¼Œæ·»åŠ æç¤º
        has_valid_reference = any(entity['label'] in answer or entity['id'] in answer for entity in valid_entities)
        if valid_entities and not has_valid_reference and 'çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°' not in answer:
            validated_answer += f"\n\nğŸ’¡ ç›¸å…³å®ä½“ï¼šåŸºäºæ‚¨çš„é—®é¢˜ï¼Œåœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°äº†ç›¸å…³å®ä½“ï¼š{', '.join([e['label'] for e in valid_entities[:3]])}ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ã€‚"
        
        return validated_answer
    
    def _extract_entity_references(self, text: str, entities: List[Dict]) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å¼•ç”¨"""
        referenced = []
        for entity in entities:
            if entity["label"].lower() in text.lower():
                referenced.append(entity["id"])
        return referenced
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _get_paginated_sources(self) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„æ¥æºä¿¡æ¯"""
        start_idx = self.current_page * self.sources_per_page
        end_idx = start_idx + self.sources_per_page
        
        page_sources = self.current_sources[start_idx:end_idx]
        total_pages = (len(self.current_sources) + self.sources_per_page - 1) // self.sources_per_page
        
        return {
            "sources": page_sources,
            "current_page": self.current_page + 1,
            "total_pages": total_pages,
            "total_sources": len(self.current_sources),
            "has_next": end_idx < len(self.current_sources),
            "has_prev": self.current_page > 0
        }
    
    def next_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸‹ä¸€é¡µæ¥æº"""
        if (self.current_page + 1) * self.sources_per_page < len(self.current_sources):
            self.current_page += 1
        return self._get_paginated_sources()
    
    def prev_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸Šä¸€é¡µæ¥æº"""
        if self.current_page > 0:
            self.current_page -= 1
        return self._get_paginated_sources()
    
    def get_chat_history(self, page: int = 1, page_size: int = None) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„èŠå¤©å†å²"""
        if page_size is None:
            page_size = AIConfig.CHAT_PAGE_SIZE
        
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        
        page_history = self.chat_history[start_idx:end_idx]
        total_pages = (len(self.chat_history) + page_size - 1) // page_size
        
        return {
            "history": page_history,
            "current_page": page,
            "total_pages": total_pages,
            "total_chats": len(self.chat_history),
            "has_next": end_idx < len(self.chat_history),
            "has_prev": page > 1
        }
    
    def clear_history(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
    
    def update_knowledge_graph(self, graph_data: Dict[str, Any]):
        """æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®"""
        self.knowledge_graph_data = graph_data
        self.entity_index = self._build_entity_index()
        print(f"[ä¿¡æ¯] çŸ¥è¯†å›¾è°±å·²æ›´æ–°ï¼ŒåŒ…å« {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹") 

    def _generate_no_knowledge_response(self, question: str) -> str:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶çš„æ ‡å‡†å›ç­”"""
        return f"""å¾ˆæŠ±æ­‰ï¼Œåœ¨å½“å‰åŒ»ç–—çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°å…³äº"{question}"çš„ç›¸å…³ä¿¡æ¯ã€‚

å½“å‰çŸ¥è¯†å›¾è°±ä¸»è¦åŒ…å«ä»¥ä¸‹ç±»å‹çš„åŒ»ç–—ä¿¡æ¯ï¼š
- ç–¾ç—…ç›¸å…³ä¿¡æ¯
- ç—‡çŠ¶æè¿°
- æ²»ç–—æ–¹æ³•
- è¯ç‰©ä¿¡æ¯
- æ£€æŸ¥é¡¹ç›®
- èº«ä½“éƒ¨ä½

å»ºè®®æ‚¨ï¼š
1. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„åŒ»ç–—æœ¯è¯­é‡æ–°æé—®
2. å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®çš„åŒ»ç–—å»ºè®®

âš ï¸ åŒ»ç–—å…è´£å£°æ˜ï¼šæœ¬ç³»ç»Ÿä»…æä¾›åŸºäºçŸ¥è¯†å›¾è°±çš„ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰å¥åº·é—®é¢˜ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚""" 