"""
åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹æ¨¡å—
"""
from typing import Dict, Any, List, Optional, Tuple
import json
import re
import requests
from collections import defaultdict
import time # Added for timing in _search_by_relation
import asyncio
import concurrent.futures

from src.config.ai_config import AIConfig, ModelType
from src.utils.graph_cache import graph_cache

class MedicalKnowledgeGraphAI:
    """åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹"""
    
    def __init__(self, knowledge_graph_data: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åŒ»ç–—AIåŠ©æ‰‹
        
        Args:
            knowledge_graph_data: çŸ¥è¯†å›¾è°±æ•°æ®ï¼ŒåŒ…å«nodeså’Œlinks
        """
        if knowledge_graph_data:
            self.knowledge_graph_data = knowledge_graph_data
        else:
            self.knowledge_graph_data = {"nodes": [], "links": []}
            
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
        self.sources_per_page = AIConfig.SOURCES_PAGE_SIZE
        
        # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å­˜ç³»ç»Ÿ
        self._use_cache = True
        
        # åˆå§‹åŒ–LLM
        self.llm = self._init_llm()
    
    def _init_llm(self):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            return self._init_ollama()
        else:
            return self._init_openai()
    
    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaæ¨¡å‹"""
        try:
            # æ£€æŸ¥OllamaæœåŠ¡å¯ç”¨æ€§ï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.get(
                f"{AIConfig.OLLAMA_BASE_URL}/api/tags", 
                timeout=5,
                proxies={'http': '', 'https': ''}
            )
            if response.status_code == 200:
                print(f"[ä¿¡æ¯] OllamaæœåŠ¡å¯ç”¨: {AIConfig.OLLAMA_BASE_URL}")
                return {"type": "ollama", "available": True}
            else:
                print(f"[è­¦å‘Š] OllamaæœåŠ¡ä¸å¯ç”¨ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {"type": "ollama", "available": False}
        except Exception as e:
            print(f"[é”™è¯¯] æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {str(e)}")
            return {"type": "ollama", "available": False}
    
    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIæ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ OpenAIçš„åˆå§‹åŒ–é€»è¾‘
        return {"type": "openai", "available": True}
    
    def _build_entity_index(self) -> Dict[str, Any]:
        """
        æ„å»ºå®ä½“ç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²åºŸå¼ƒï¼Œç°åœ¨ä½¿ç”¨graph_cacheä¸­çš„ä¼˜åŒ–ç´¢å¼•ç³»ç»Ÿ
        """
        print("[è­¦å‘Š] _build_entity_indexæ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨graph_cacheä¼˜åŒ–ç´¢å¼•ç³»ç»Ÿ")
        index = {
            "entities": {},  # å®ä½“IDåˆ°å®ä½“ä¿¡æ¯çš„æ˜ å°„
            "relationships": defaultdict(list),  # å®ä½“é—´å…³ç³»
            "search_index": {}  # æœç´¢ç´¢å¼•ï¼ˆå®ä½“åç§°åˆ°IDçš„æ˜ å°„ï¼‰
        }
        
        # æ„å»ºèŠ‚ç‚¹ç´¢å¼•
        for node in self.knowledge_graph_data.get("nodes", []):
            entity_id = node.get("id")
            entity_label = node.get("label", "")
            
            index["entities"][entity_id] = {
                "id": entity_id,
                "label": entity_label,
                "type": node.get("type", "entity"),
                "connections": node.get("connections", 0)
            }
            
            # æ„å»ºæœç´¢ç´¢å¼•
            if entity_label:
                # æ”¯æŒæ¨¡ç³Šæœç´¢
                label_lower = entity_label.lower()
                index["search_index"][label_lower] = entity_id
                
                # æ·»åŠ éƒ¨åˆ†åŒ¹é…
                words = label_lower.split()
                for word in words:
                    if len(word) > 1:  # é¿å…å•å­—ç¬¦ç´¢å¼•
                        if word not in index["search_index"]:
                            index["search_index"][word] = []
                        if isinstance(index["search_index"][word], list):
                            index["search_index"][word].append(entity_id)
                        else:
                            index["search_index"][word] = [index["search_index"][word], entity_id]
        
        # æ„å»ºå…³ç³»ç´¢å¼•
        for link in self.knowledge_graph_data.get("links", []):
            source = link.get("source")
            target = link.get("target")
            relation = link.get("label", "ç›¸å…³")
            
            if source and target:
                index["relationships"][source].append({
                    "target": target,
                    "relation": relation,
                    "direction": "outgoing"
                })
                index["relationships"][target].append({
                    "target": source,
                    "relation": relation,
                    "direction": "incoming"
                })
        
        return index
    
    def search_entities(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿæœç´¢å®ä½“ - ä½¿ç”¨ä¼˜åŒ–çš„ç´¢å¼•ç®—æ³•
        æ—¶é—´å¤æ‚åº¦ä» O(EÃ—QÃ—W) é™ä½åˆ° O(log N)
        """
        if not query.strip():
            return []
        
        print(f"[è°ƒè¯•] æœç´¢æŸ¥è¯¢: {query}, é™åˆ¶: {limit}") # è°ƒè¯•ä¿¡æ¯
        
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„å¿«é€Ÿæœç´¢
        if self._use_cache and graph_cache.get_cached_graph():
            results = graph_cache.search_entities_fast(query, limit)
            print(f"[è°ƒè¯•] ç¼“å­˜æœç´¢ç»“æœæ•°é‡: {len(results)}") # è°ƒè¯•ä¿¡æ¯
            return results
        
        # å¤‡ç”¨ï¼šåŸå§‹æœç´¢ç®—æ³•
        results = self._search_entities_fallback(query, limit)
        print(f"[è°ƒè¯•] å¤‡ç”¨æœç´¢ç»“æœæ•°é‡: {len(results)}") # è°ƒè¯•ä¿¡æ¯
        return results
    
    def _search_entities_fallback(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """å¤‡ç”¨æœç´¢ç®—æ³• - å½“ç¼“å­˜ä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        query_lower = query.lower().strip()
        if not query_lower:
            return []
        
        scored_results = []
        
        # ç®€åŒ–çš„åŒ»ç–—æœ¯è¯­æ˜ å°„
        medical_terms = {
            'æ„Ÿå†’': ['æ„Ÿå†’', 'æ™®é€šæ„Ÿå†’', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“'],
            'å‘çƒ§': ['å‘çƒ­', 'å‘çƒ§', 'ä½“æ¸©å‡é«˜'],
            'å’³å—½': ['å’³å—½', 'å’³ç—°', 'å¹²å’³'],
            'å¤´ç—›': ['å¤´ç—›', 'å¤´ç–¼', 'åå¤´ç—›']
        }
        
        # æ‰©å±•æŸ¥è¯¢è¯
        expanded_queries = [query_lower]
        for term, synonyms in medical_terms.items():
            if term in query_lower:
                expanded_queries.extend(synonyms)
        
        # å¿«é€ŸåŒ¹é…è¯„åˆ†
        for node in self.knowledge_graph_data.get("nodes", []):
            entity_label = node.get("label", "").lower()
            score = 0
            match_type = "none"
            
            # ç²¾ç¡®åŒ¹é…
            if query_lower == entity_label:
                score = 100
                match_type = "exact"
            # åŒ…å«åŒ¹é…
            elif query_lower in entity_label or entity_label in query_lower:
                score = 80
                match_type = "contains"
            # è¯è¯­åŒ¹é…
            else:
                query_words = set(query_lower.split())
                entity_words = set(entity_label.split())
                intersection = query_words & entity_words
                if intersection:
                    score = 60 * len(intersection) / len(query_words)
                    match_type = "partial"
            
            if score > 0:
                result_entity = {
                    **node,
                    "match_type": match_type,
                    "match_score": score
                }
                scored_results.append(result_entity)
        
        # æ’åºå¹¶è¿”å›
        scored_results.sort(key=lambda x: (x["match_score"], x.get("connections", 0)), reverse=True)
        return scored_results[:limit]
    
    def get_entity_context(self, entity_id: str, depth: int = 1) -> Dict[str, Any]:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„å®ä½“ä¿¡æ¯
        if self._use_cache and graph_cache.get_cached_graph():
            cached_graph = graph_cache.get_cached_graph()
            if not cached_graph:
                return {}
                
            entity = None
            
            # æŸ¥æ‰¾å®ä½“
            for node in cached_graph.get('nodes', []):
                if node.get('id') == entity_id:
                    entity = node
                    break
            
            if not entity:
                return {}
            
            context = {
                "entity": entity,
                "relationships": [],
                "neighbors": []
            }
            
            # è·å–å…³ç³»
            for edge in cached_graph.get('edges', []):
                if edge['source'] == entity_id:
                    # æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹
                    for node in cached_graph.get('nodes', []):
                        if node.get('id') == edge['target']:
                            context["relationships"].append({
                                "relation": edge['relation'],
                                "direction": "outgoing",
                                "neighbor": node
                            })
                            context["neighbors"].append(node)
                            break
                elif edge['target'] == entity_id:
                    # æŸ¥æ‰¾æºèŠ‚ç‚¹
                    for node in cached_graph.get('nodes', []):
                        if node.get('id') == edge['source']:
                            context["relationships"].append({
                                "relation": edge['relation'],
                                "direction": "incoming",
                                "neighbor": node
                            })
                            context["neighbors"].append(node)
                            break
            
            return context
        
        # å¤‡ç”¨ï¼šç›´æ¥ä»çŸ¥è¯†å›¾è°±æ•°æ®æŸ¥æ‰¾
        entity = None
        for node in self.knowledge_graph_data.get('nodes', []):
            if node.get('id') == entity_id:
                entity = node
                break
        
        if not entity:
            return {}
        
        context = {
            "entity": entity,
            "relationships": [],
            "neighbors": []
        }
        
        # è·å–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for edge in self.knowledge_graph_data.get('edges', []):
            if edge['source'] == entity_id or edge['target'] == entity_id:
                neighbor_id = edge['target'] if edge['source'] == entity_id else edge['source']
                direction = "outgoing" if edge['source'] == entity_id else "incoming"
                
                # æŸ¥æ‰¾é‚»å±…èŠ‚ç‚¹
                for node in self.knowledge_graph_data.get('nodes', []):
                    if node.get('id') == neighbor_id:
                        context["relationships"].append({
                            "relation": edge['relation'],
                            "direction": direction,
                            "neighbor": node
                        })
                        context["neighbors"].append(node)
                        break
        
        return context
    
    def _call_ollama(self, prompt: str, context: str = "") -> str:
        """è°ƒç”¨Ollamaæ¨¡å‹"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            if context:
                full_prompt += f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼š\n{context}\n\n"
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\nè¯·åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            
            # è°ƒç”¨Ollama APIï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False
                },
                timeout=30,
                proxies={'http': '', 'https': ''}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def ask(self, question: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›å›ç­”"""
        if not question.strip():
            return {
                "answer": "è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜ã€‚",
                "related_entities": [],
                "suggested_focus": None,
                "sources": self._get_paginated_sources(),
                "medical_disclaimer": True
            }
        
        # è§£ææŸ¥è¯¢æ„å›¾
        query_intent = self._parse_query_intent(question)
        print(f"[è°ƒè¯•] æŸ¥è¯¢æ„å›¾: {query_intent}")
        
        # ä½¿ç”¨å¹¶å‘æœç´¢æå‡æ€§èƒ½
        related_entities = self._search_concurrent(query_intent, limit=8)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å®ä½“ï¼Œç›´æ¥è¿”å›æ ‡å‡†å›ç­”
        if not related_entities:
            answer = self._generate_no_knowledge_response(question)
            # æ¸…ç©ºå½“å‰æ¥æº
            self.current_sources = []
            self.current_page = 0
            
            return {
                "answer": answer,
                "related_entities": [],
                "suggested_focus": None,
                "sources": self._get_paginated_sources(),
                "medical_disclaimer": True,
                "context_used": "æœªæ‰¾åˆ°ç›¸å…³å®ä½“",
                "knowledge_graph_coverage": False
            }
        
        # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        if related_entities:
            context_info.append("=== çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ ===")
            for entity in related_entities:
                entity_context = self.get_entity_context(entity["id"])
                context_info.append(f"\nå®ä½“: {entity['label']}")
                context_info.append(f"ID: {entity['id']}")
                context_info.append(f"ç±»å‹: {entity.get('type', 'æœªçŸ¥')}")
                context_info.append(f"è¿æ¥æ•°: {entity.get('connections', 0)}")
                
                # æ·»åŠ å…³ç³»ä¿¡æ¯
                relationships = entity_context.get("relationships", [])
                if relationships:
                    context_info.append("å…³ç³»:")
                    for rel in relationships[:5]:  # é™åˆ¶å…³ç³»æ•°é‡
                        direction_symbol = "â†’" if rel['direction'] == 'outgoing' else "â†"
                        context_info.append(f"  {direction_symbol} {rel['relation']}: {rel['neighbor']['label']} (ID: {rel['neighbor']['id']})")
                else:
                    context_info.append("å…³ç³»: æ— ç›´æ¥å…³ç³»")
                    
            context_info.append("\n=== ä¸Šä¸‹æ–‡ç»“æŸ ===")
        
        context_text = "\n".join(context_info) if context_info else "æœªæ‰¾åˆ°ç›¸å…³å®ä½“"
        
        # è°ƒç”¨AIæ¨¡å‹
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            answer = self._call_ollama_strict(question, context_text, related_entities)
        else:
            answer = "OpenAIæ¨¡å‹æš‚æœªå®ç°"
        
        # éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨
        validated_answer = self._validate_entity_references(answer, related_entities)
        
        # å»ºè®®èšç„¦çš„èŠ‚ç‚¹ï¼ˆé€‰æ‹©æœ€ç›¸å…³çš„å®ä½“ï¼‰
        suggested_focus = related_entities[0]["id"] if related_entities else None
        
        # ä¿å­˜åˆ°èŠå¤©å†å²
        self.chat_history.append({
            "question": question,
            "answer": validated_answer,
            "related_entities": related_entities,
            "context_used": context_text,
            "timestamp": self._get_timestamp()
        })
        
        # æ›´æ–°å½“å‰æ¥æº
        self.current_sources = related_entities
        self.current_page = 0
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"[è°ƒè¯•] ç›¸å…³å®ä½“æ•°é‡: {len(related_entities)}")
        print(f"[è°ƒè¯•] å½“å‰æ¥æºæ•°é‡: {len(self.current_sources)}")
        print(f"[è°ƒè¯•] åˆ†é¡µæ¥æº: {self._get_paginated_sources()}")
        
        return {
            "answer": validated_answer,
            "related_entities": related_entities,
            "suggested_focus": suggested_focus,
            "sources": self._get_paginated_sources(),
            "medical_disclaimer": True,
            "context_used": context_text,
            "knowledge_graph_coverage": len(related_entities) > 0
        }

    def _call_ollama_strict(self, prompt: str, context: str = "", entities: Optional[List[Dict]] = None) -> str:
        """ä¸¥æ ¼è°ƒç”¨Ollamaæ¨¡å‹ï¼Œå¼ºåŒ–çŸ¥è¯†å›¾è°±çº¦æŸ"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºä¸¥æ ¼çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            
            if context and entities:
                full_prompt += f"{context}\n\n"
                full_prompt += "ã€å¯å¼•ç”¨çš„å®ä½“IDåˆ—è¡¨ã€‘ï¼š\n"
                for entity in entities:
                    full_prompt += f"- {entity['label']} (ID: {entity['id']})\n"
                full_prompt += "\n"
                full_prompt += "âš ï¸ é‡è¦çº¦æŸï¼š\n"
                full_prompt += "1. åªèƒ½ä½¿ç”¨ä¸Šè¿°çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¿¡æ¯\n"
                full_prompt += "2. ä¸¥ç¦ç¼–é€ æˆ–æ¨æµ‹ä»»ä½•ä¸åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“\n"
                full_prompt += "3. å¦‚æœçŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜'çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯'\n"
                full_prompt += "4. å›ç­”ä¸­æåˆ°çš„æ¯ä¸ªå®ä½“éƒ½å¿…é¡»åœ¨ä¸Šè¿°åˆ—è¡¨ä¸­\n"
                full_prompt += "5. ä¸è¦ä½¿ç”¨ä»»ä½•è®­ç»ƒæ•°æ®æˆ–å¸¸è¯†çŸ¥è¯†\n\n"
            else:
                full_prompt += "çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼šæ— ç›¸å…³å®ä½“\n\n"
                full_prompt += "âš ï¸ é‡è¦çº¦æŸï¼š\n"
                full_prompt += "1. çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³å®ä½“\n"
                full_prompt += "2. ä¸¥ç¦ç¼–é€ æˆ–æ¨æµ‹ä»»ä½•å®ä½“\n"
                full_prompt += "3. æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯\n\n"
            
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\n"
            full_prompt += "è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°çº¦æŸå›ç­”ï¼Œåªèƒ½ä½¿ç”¨ä¸Šè¿°ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å’Œå®ä½“IDã€‚å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # è°ƒç”¨Ollama APIï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # é™ä½æ¸©åº¦ï¼Œå‡å°‘åˆ›é€ æ€§
                        "top_p": 0.8,
                        "top_k": 10
                    }
                },
                timeout=30,
                proxies={'http': '', 'https': ''}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
                
                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«æœªæˆæƒçš„å®ä½“
                if entities:
                    valid_labels = set(entity['label'] for entity in entities)
                    valid_ids = set(entity['id'] for entity in entities)
                    
                    # ç®€å•çš„å®ä½“æ£€æµ‹ï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
                    import re
                    # æ£€æµ‹å¯èƒ½çš„å®ä½“IDæ¨¡å¼
                    id_pattern = r'\b[A-Z]\d+\b'
                    found_ids = re.findall(id_pattern, answer)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœªæˆæƒçš„å®ä½“ID
                    unauthorized_ids = [found_id for found_id in found_ids if found_id not in valid_ids]
                    if unauthorized_ids:
                        answer = f"âš ï¸ æ£€æµ‹åˆ°æœªæˆæƒçš„å®ä½“å¼•ç”¨ï¼Œå·²ç§»é™¤ã€‚\n\n{answer}"
                
                return answer
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"

    def _validate_entity_references(self, answer: str, valid_entities: List[Dict]) -> str:
        """éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨ï¼Œç§»é™¤æ— æ•ˆçš„å®ä½“ID"""
        if not valid_entities:
            return answer
        
        # æå–æœ‰æ•ˆçš„å®ä½“ID
        valid_ids = set(entity['id'] for entity in valid_entities)
        valid_labels = set(entity['label'] for entity in valid_entities)
        
        # æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«æ— æ•ˆçš„å®ä½“IDæ¨¡å¼
        import re
        
        # æŸ¥æ‰¾å¯èƒ½çš„å®ä½“IDæ¨¡å¼ï¼ˆå¦‚ D123, F456, B001 ç­‰ï¼‰
        id_pattern = r'\b[A-Z]\d+\b'
        found_ids = re.findall(id_pattern, answer)
        
        # ç§»é™¤æ— æ•ˆçš„å®ä½“ID
        validated_answer = answer
        for found_id in found_ids:
            if found_id not in valid_ids:
                # ç§»é™¤æ— æ•ˆçš„å®ä½“IDå¼•ç”¨
                validated_answer = re.sub(rf'\([^)]*{re.escape(found_id)}[^)]*\)', '', validated_answer)
                validated_answer = re.sub(rf'\b{re.escape(found_id)}\b', '', validated_answer)
        
        # å¦‚æœå›ç­”ä¸­æ²¡æœ‰å¼•ç”¨ä»»ä½•æœ‰æ•ˆå®ä½“ä¸”åŸæœ¬æœ‰ç›¸å…³å®ä½“ï¼Œæ·»åŠ æç¤º
        has_valid_reference = any(entity['label'] in answer or entity['id'] in answer for entity in valid_entities)
        if valid_entities and not has_valid_reference and 'çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°' not in answer:
            validated_answer += f"\n\nğŸ’¡ ç›¸å…³å®ä½“ï¼šåŸºäºæ‚¨çš„é—®é¢˜ï¼Œåœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°äº†ç›¸å…³å®ä½“ï¼š{', '.join([e['label'] for e in valid_entities[:3]])}ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ã€‚"
        
        return validated_answer
    
    def _extract_entity_references(self, text: str, entities: List[Dict]) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å¼•ç”¨"""
        referenced = []
        for entity in entities:
            if entity["label"].lower() in text.lower():
                referenced.append(entity["id"])
        return referenced
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _get_paginated_sources(self) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„æ¥æºä¿¡æ¯"""
        start_idx = self.current_page * self.sources_per_page
        end_idx = start_idx + self.sources_per_page
        
        page_sources = self.current_sources[start_idx:end_idx]
        total_pages = (len(self.current_sources) + self.sources_per_page - 1) // self.sources_per_page
        
        return {
            "sources": page_sources,
            "current_page": self.current_page + 1,
            "total_pages": total_pages,
            "total_sources": len(self.current_sources),
            "has_next": end_idx < len(self.current_sources),
            "has_prev": self.current_page > 0
        }
    
    def next_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸‹ä¸€é¡µæ¥æº"""
        if (self.current_page + 1) * self.sources_per_page < len(self.current_sources):
            self.current_page += 1
        return self._get_paginated_sources()
    
    def prev_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸Šä¸€é¡µæ¥æº"""
        if self.current_page > 0:
            self.current_page -= 1
        return self._get_paginated_sources()
    
    def get_chat_history(self, page: int = 1, page_size: Optional[int] = None) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„èŠå¤©å†å²"""
        # ç¡®ä¿page_sizeæ˜¯intç±»å‹
        actual_page_size: int = page_size if page_size is not None else AIConfig.CHAT_PAGE_SIZE
        
        start_idx = (page - 1) * actual_page_size
        end_idx = start_idx + actual_page_size
        
        page_history = self.chat_history[start_idx:end_idx]
        total_pages = (len(self.chat_history) + actual_page_size - 1) // actual_page_size
        
        return {
            "history": page_history,
            "current_page": page,
            "total_pages": total_pages,
            "total_chats": len(self.chat_history),
            "has_next": end_idx < len(self.chat_history),
            "has_prev": page > 1
        }
    
    def clear_history(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
    
    def update_knowledge_graph(self, graph_data: Dict[str, Any]):
        """æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®"""
        self.knowledge_graph_data = graph_data
        # ä¸å†ä½¿ç”¨entity_indexï¼Œæ”¹ç”¨ç¼“å­˜ç³»ç»Ÿ
        print(f"[ä¿¡æ¯] çŸ¥è¯†å›¾è°±å·²æ›´æ–°ï¼ŒåŒ…å« {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹") 

    def update_knowledge_graph_from_file(self, csv_file_path: str, force_reload: bool = False):
        """ä»CSVæ–‡ä»¶æ›´æ–°çŸ¥è¯†å›¾è°±ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å­˜åŠ è½½
            graph_data = graph_cache.load_graph(csv_file_path, force_reload)
            self.knowledge_graph_data = graph_data
            print(f"[ä¿¡æ¯] çŸ¥è¯†å›¾è°±å·²æ›´æ–°ï¼ŒåŒ…å« {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            print(f"[é”™è¯¯] æ›´æ–°çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            self.knowledge_graph_data = {"nodes": [], "links": []}

    def _generate_no_knowledge_response(self, question: str) -> str:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶çš„æ ‡å‡†å›ç­”"""
        return f"""å¾ˆæŠ±æ­‰ï¼Œåœ¨å½“å‰åŒ»ç–—çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°å…³äº"{question}"çš„ç›¸å…³ä¿¡æ¯ã€‚

å½“å‰çŸ¥è¯†å›¾è°±ä¸»è¦åŒ…å«ä»¥ä¸‹ç±»å‹çš„åŒ»ç–—ä¿¡æ¯ï¼š
- ç–¾ç—…ç›¸å…³ä¿¡æ¯
- ç—‡çŠ¶æè¿°
- æ²»ç–—æ–¹æ³•
- è¯ç‰©ä¿¡æ¯
- æ£€æŸ¥é¡¹ç›®
- èº«ä½“éƒ¨ä½

å»ºè®®æ‚¨ï¼š
1. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„åŒ»ç–—æœ¯è¯­é‡æ–°æé—®
2. å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®çš„åŒ»ç–—å»ºè®®

âš ï¸ åŒ»ç–—å…è´£å£°æ˜ï¼šæœ¬ç³»ç»Ÿä»…æä¾›åŸºäºçŸ¥è¯†å›¾è°±çš„ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰å¥åº·é—®é¢˜ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚""" 

    def _parse_query_intent(self, query: str) -> Dict[str, Any]:
        """è§£ææŸ¥è¯¢æ„å›¾ï¼Œæå–ç–¾ç—…ã€å…³ç³»å’Œç›®æ ‡"""
        query_lower = query.lower().strip()
        
        # å®šä¹‰æŸ¥è¯¢æ¨¡å¼
        patterns = {
            # é¥®é£Ÿç›¸å…³
            'diet': {
                'keywords': ['åƒä»€ä¹ˆ', 'é¥®é£Ÿ', 'é£Ÿç‰©', 'èœ', 'æ±¤', 'ç²¥', 'æ°´æœ', 'è”¬èœ', 'è¥å…»'],
                'relation': 'æ¨èé£Ÿè°±',
                'target_type': 'food'
            },
            # è¯ç‰©ç›¸å…³
            'medicine': {
                'keywords': ['åƒä»€ä¹ˆè¯', 'è¯ç‰©', 'è¯å“', 'è¯', 'æ²»ç–—', 'ç”¨è¯', 'å¤„æ–¹'],
                'relation': 'å¸¸ç”¨è¯å“',
                'target_type': 'medicine'
            },
            # ç—‡çŠ¶ç›¸å…³
            'symptoms': {
                'keywords': ['ç—‡çŠ¶', 'è¡¨ç°', 'å¾å…†', 'æ„Ÿè§‰', 'ä¸é€‚'],
                'relation': 'ç—‡çŠ¶',
                'target_type': 'symptom'
            },
            # æ£€æŸ¥ç›¸å…³
            'examination': {
                'keywords': ['æ£€æŸ¥', 'åŒ–éªŒ', 'æ£€æµ‹', 'è¯Šæ–­', 'ç­›æŸ¥'],
                'relation': 'æ£€æŸ¥é¡¹ç›®',
                'target_type': 'examination'
            },
            # é¢„é˜²ç›¸å…³
            'prevention': {
                'keywords': ['é¢„é˜²', 'é¿å…', 'é˜²æ­¢', 'é¢„é˜²æªæ–½'],
                'relation': 'é¢„é˜²æªæ–½',
                'target_type': 'prevention'
            },
            # å¹¶å‘ç—‡ç›¸å…³
            'complications': {
                'keywords': ['å¹¶å‘ç—‡', 'åæœ', 'å½±å“', 'æ¶åŒ–'],
                'relation': 'å¹¶å‘ç—‡',
                'target_type': 'complication'
            },
            # ç—‡çŠ¶è¯Šæ–­ç›¸å…³
            'diagnosis': {
                'keywords': ['å¯èƒ½æ˜¯ä»€ä¹ˆç—…', 'ä»€ä¹ˆç—…', 'è¯Šæ–­', 'ä»€ä¹ˆåŸå› ', 'æ€ä¹ˆå›äº‹'],
                'relation': 'ç—‡çŠ¶',
                'target_type': 'diagnosis'
            }
        }
        
        # æ£€æµ‹æŸ¥è¯¢æ„å›¾
        detected_intent = None
        for intent, pattern in patterns.items():
            if any(keyword in query_lower for keyword in pattern['keywords']):
                detected_intent = intent
                break
        
        # æå–ç–¾ç—…åç§°
        disease_keywords = ['æ„Ÿå†’', 'å‘çƒ§', 'å’³å—½', 'å¤´ç—›', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å¿ƒè„ç—…', 'è‚ºç‚', 'èƒƒç‚', 'è‚ç‚']
        detected_disease = None
        for disease in disease_keywords:
            if disease in query_lower:
                detected_disease = disease
                break
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç—‡çŠ¶è¯Šæ–­æŸ¥è¯¢
        is_symptom_diagnosis = self._is_symptom_diagnosis_query(query_lower)
        
        return {
            'original_query': query,
            'intent': detected_intent,
            'disease': detected_disease,
            'relation': patterns[detected_intent]['relation'] if detected_intent else None,
            'target_type': patterns[detected_intent]['target_type'] if detected_intent else None,
            'is_structured_query': detected_intent is not None and detected_disease is not None,
            'is_symptom_diagnosis': is_symptom_diagnosis,
            'symptoms': self._extract_symptoms(query_lower) if is_symptom_diagnosis else []
        }
    
    def _is_symptom_diagnosis_query(self, query_lower: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç—‡çŠ¶è¯Šæ–­æŸ¥è¯¢"""
        # è¯Šæ–­ç›¸å…³å…³é”®è¯
        diagnosis_keywords = [
            'å¯èƒ½æ˜¯ä»€ä¹ˆç—…', 'ä»€ä¹ˆç—…', 'è¯Šæ–­', 'ä»€ä¹ˆåŸå› ', 'æ€ä¹ˆå›äº‹',
            'æˆ‘æœ‰ç‚¹', 'æˆ‘æœ‰', 'æˆ‘å‡ºç°', 'æˆ‘å¾—äº†', 'æˆ‘æ‚£äº†',
            'ç—‡çŠ¶', 'è¡¨ç°', 'å¾å…†', 'æ„Ÿè§‰', 'ä¸é€‚'
        ]
        
        # ç—‡çŠ¶å…³é”®è¯
        symptom_keywords = [
            'æ„Ÿå†’', 'å‘çƒ§', 'å‘çƒ­', 'å’³å—½', 'å¤´ç—›', 'å¤´ç–¼', 'æµé¼»æ¶•', 'é¼»å¡',
            'å–‰å’™ç—›', 'å’½ç—›', 'å—“å­ç–¼', 'æ‰“å–·åš', 'ä¹åŠ›', 'ç–²åŠ³', 'é£Ÿæ¬²ä¸æŒ¯',
            'æ¶å¿ƒ', 'å‘•å', 'è…¹æ³»', 'è…¹ç—›', 'è…¹èƒ€', 'ä¾¿ç§˜', 'å¤±çœ ', 'å¤šæ¢¦',
            'å¿ƒæ‚¸', 'èƒ¸é—·', 'æ°”çŸ­', 'å‘¼å¸å›°éš¾', 'èƒ¸ç—›', 'èƒŒç—›', 'å…³èŠ‚ç—›',
            'è‚Œè‚‰é…¸ç—›', 'çš®ç–¹', 'ç˜™ç—’', 'çº¢è‚¿', 'æ°´è‚¿', 'å¤´æ™•', 'çœ©æ™•',
            'è€³é¸£', 'è§†åŠ›æ¨¡ç³Š', 'çœ¼ç—›', 'çœ¼çº¢', 'æµæ³ª', 'å£å¹²', 'å£è‹¦',
            'å£è‡­', 'ç‰™é¾ˆå‡ºè¡€', 'ç‰™ç—›', 'å£è…”æºƒç–¡', 'å£°éŸ³å˜¶å“‘', 'å¤±å£°'
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯Šæ–­å…³é”®è¯å’Œç—‡çŠ¶å…³é”®è¯
        has_diagnosis_keyword = any(keyword in query_lower for keyword in diagnosis_keywords)
        has_symptom_keyword = any(keyword in query_lower for keyword in symptom_keywords)
        
        return has_diagnosis_keyword and has_symptom_keyword
    
    def _extract_symptoms(self, query_lower: str) -> List[str]:
        """æå–ç—‡çŠ¶å…³é”®è¯"""
        # ç—‡çŠ¶æ˜ å°„è¡¨
        symptom_mapping = {
            'æ„Ÿå†’': ['æ„Ÿå†’', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“'],
            'å‘çƒ§': ['å‘çƒ§', 'å‘çƒ­', 'ä½“æ¸©å‡é«˜'],
            'å’³å—½': ['å’³å—½', 'å’³ç—°', 'å¹²å’³'],
            'å¤´ç—›': ['å¤´ç—›', 'å¤´ç–¼', 'åå¤´ç—›'],
            'æµé¼»æ¶•': ['æµé¼»æ¶•', 'é¼»æ¶•', 'é¼»å¡'],
            'å–‰å’™ç—›': ['å–‰å’™ç—›', 'å’½ç—›', 'å—“å­ç–¼'],
            'æ‰“å–·åš': ['æ‰“å–·åš', 'å–·åš'],
            'ä¹åŠ›': ['ä¹åŠ›', 'ç–²åŠ³', 'æ— åŠ›'],
            'é£Ÿæ¬²ä¸æŒ¯': ['é£Ÿæ¬²ä¸æŒ¯', 'ä¸æƒ³åƒé¥­', 'æ²¡èƒƒå£'],
            'æ¶å¿ƒ': ['æ¶å¿ƒ', 'æƒ³å'],
            'å‘•å': ['å‘•å', 'å'],
            'è…¹æ³»': ['è…¹æ³»', 'æ‹‰è‚šå­'],
            'è…¹ç—›': ['è…¹ç—›', 'è‚šå­ç–¼'],
            'è…¹èƒ€': ['è…¹èƒ€', 'è‚šå­èƒ€'],
            'ä¾¿ç§˜': ['ä¾¿ç§˜', 'å¤§ä¾¿å¹²ç‡¥'],
            'å¤±çœ ': ['å¤±çœ ', 'ç¡ä¸ç€'],
            'å¿ƒæ‚¸': ['å¿ƒæ‚¸', 'å¿ƒè·³å¿«'],
            'èƒ¸é—·': ['èƒ¸é—·', 'èƒ¸å£é—·'],
            'æ°”çŸ­': ['æ°”çŸ­', 'å‘¼å¸å›°éš¾'],
            'èƒ¸ç—›': ['èƒ¸ç—›', 'èƒ¸å£ç–¼'],
            'èƒŒç—›': ['èƒŒç—›', 'åèƒŒç–¼'],
            'å…³èŠ‚ç—›': ['å…³èŠ‚ç—›', 'å…³èŠ‚ç–¼'],
            'è‚Œè‚‰é…¸ç—›': ['è‚Œè‚‰é…¸ç—›', 'è‚Œè‚‰ç–¼'],
            'çš®ç–¹': ['çš®ç–¹', 'çº¢ç–¹'],
            'ç˜™ç—’': ['ç˜™ç—’', 'ç—’'],
            'çº¢è‚¿': ['çº¢è‚¿', 'çº¢'],
            'æ°´è‚¿': ['æ°´è‚¿', 'è‚¿'],
            'å¤´æ™•': ['å¤´æ™•', 'æ™•'],
            'çœ©æ™•': ['çœ©æ™•', 'å¤©æ—‹åœ°è½¬'],
            'è€³é¸£': ['è€³é¸£', 'è€³æœµå“'],
            'è§†åŠ›æ¨¡ç³Š': ['è§†åŠ›æ¨¡ç³Š', 'çœ‹ä¸æ¸…'],
            'çœ¼ç—›': ['çœ¼ç—›', 'çœ¼ç›ç–¼'],
            'çœ¼çº¢': ['çœ¼çº¢', 'çœ¼ç›çº¢'],
            'æµæ³ª': ['æµæ³ª', 'çœ¼æ³ª'],
            'å£å¹²': ['å£å¹²', 'å˜´å·´å¹²'],
            'å£è‹¦': ['å£è‹¦', 'å˜´å·´è‹¦'],
            'å£è‡­': ['å£è‡­', 'å£æ°”'],
            'ç‰™é¾ˆå‡ºè¡€': ['ç‰™é¾ˆå‡ºè¡€', 'ç‰™é¾ˆ'],
            'ç‰™ç—›': ['ç‰™ç—›', 'ç‰™é½¿ç–¼'],
            'å£è…”æºƒç–¡': ['å£è…”æºƒç–¡', 'æºƒç–¡'],
            'å£°éŸ³å˜¶å“‘': ['å£°éŸ³å˜¶å“‘', 'å˜¶å“‘'],
            'å¤±å£°': ['å¤±å£°', 'è¯´ä¸å‡ºè¯']
        }
        
        extracted_symptoms = []
        for symptom, keywords in symptom_mapping.items():
            if any(keyword in query_lower for keyword in keywords):
                extracted_symptoms.append(symptom)
        
        return extracted_symptoms
    
    def _search_by_relation(self, disease: str, relation: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æ ¹æ®ç–¾ç—…å’Œå…³ç³»æœç´¢ç›¸å…³å®ä½“"""
        if not disease or not relation:
            return []
        
        print(f"[è°ƒè¯•] å…³ç³»æœç´¢: ç–¾ç—…={disease}, å…³ç³»={relation}")
        start_time = time.time()
        
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„å¿«é€Ÿå…³ç³»æœç´¢
        if self._use_cache and graph_cache.get_cached_graph():
            results = graph_cache.search_by_relation_fast(disease, relation, limit)
            end_time = time.time()
            print(f"[è°ƒè¯•] å¿«é€Ÿå…³ç³»æœç´¢å®Œæˆ, è€—æ—¶ {end_time - start_time:.3f}s")
            return results
        
        # å¤‡ç”¨ï¼šåŸå§‹æœç´¢ç®—æ³•
        results = []
        
        # æŸ¥æ‰¾ç–¾ç—…å®ä½“
        disease_entities = []
        for node in self.knowledge_graph_data.get("nodes", []):
            if disease in node.get("label", ""):
                disease_entities.append(node)
        
        print(f"[è°ƒè¯•] æ‰¾åˆ°ç–¾ç—…å®ä½“æ•°é‡: {len(disease_entities)}")
        
        for disease_entity in disease_entities:
            # æŸ¥æ‰¾ç›¸å…³å…³ç³»
            for edge in self.knowledge_graph_data.get("edges", []):
                if edge.get('source') == disease_entity['id']:
                    edge_relation = edge.get('relation', '')
                    # çµæ´»çš„å…³ç³»åŒ¹é…
                    if (relation in edge_relation or 
                        edge_relation in relation or
                        any(keyword in edge_relation for keyword in relation.split())):
                        
                        # æ‰¾åˆ°ç›®æ ‡å®ä½“
                        target_id = edge.get('target')
                        for node in self.knowledge_graph_data.get("nodes", []):
                            if node.get('id') == target_id:
                                result = {
                                    **node,
                                    "match_type": "relation",
                                    "match_score": 90,
                                    "relation": edge_relation,
                                    "source_disease": disease_entity['label'],
                                    "edge_id": edge.get('id'),
                                    "search_method": "fallback_search"
                                }
                                results.append(result)
                                break
        
        end_time = time.time()
        print(f"[è°ƒè¯•] å…³ç³»æœç´¢ç»“æœæ•°é‡: {len(results)}, è€—æ—¶ {end_time - start_time:.3f}s")
        return results[:limit] 

    def _search_by_symptoms(self, symptoms: List[str], limit: int = 10) -> List[Dict[str, Any]]:
        """æ ¹æ®ç—‡çŠ¶æœç´¢å¯èƒ½çš„ç–¾ç—…"""
        if not symptoms:
            return []
        
        print(f"[è°ƒè¯•] ç—‡çŠ¶è¯Šæ–­æœç´¢: ç—‡çŠ¶={symptoms}")
        start_time = time.time()
        
        results = []
        seen_ids = set()
        
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
        if self._use_cache and graph_cache.get_cached_graph():
            cached_graph = graph_cache.get_cached_graph()
            
            # é€šè¿‡ç—‡çŠ¶å…³é”®è¯æœç´¢ç›¸å…³ç–¾ç—…
            for symptom in symptoms:
                # æœç´¢åŒ…å«ç—‡çŠ¶å…³é”®è¯çš„å®ä½“
                for node in cached_graph.get('nodes', []):
                    node_label = node.get('label', '').lower()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç–¾ç—…å®ä½“ä¸”åŒ…å«ç—‡çŠ¶ä¿¡æ¯
                    if (symptom in node_label and 
                        any(disease_keyword in node_label for disease_keyword in ['æ„Ÿå†’', 'å‘çƒ§', 'å’³å—½', 'å¤´ç—›', 'è‚ºç‚', 'èƒƒç‚', 'è‚ç‚', 'é«˜è¡€å‹', 'ç³–å°¿ç—…'])):
                        
                        if node.get('id') not in seen_ids:
                            result = {
                                **node,
                                "match_type": "symptom_diagnosis",
                                "match_score": 85,
                                "matched_symptoms": [symptom],
                                "search_method": "symptom_disease_match"
                            }
                            results.append(result)
                            seen_ids.add(node.get('id'))
                            if len(results) >= limit:
                                break
                
                if len(results) >= limit:
                    break
            
            # å¦‚æœç—‡çŠ¶åŒ¹é…ä¸å¤Ÿï¼Œæœç´¢ç—‡çŠ¶ç›¸å…³çš„å®ä½“
            if len(results) < limit:
                for symptom in symptoms:
                    for node in cached_graph.get('nodes', []):
                        node_label = node.get('label', '').lower()
                        
                        if symptom in node_label and node.get('id') not in seen_ids:
                            result = {
                                **node,
                                "match_type": "symptom_entity",
                                "match_score": 70,
                                "matched_symptoms": [symptom],
                                "search_method": "symptom_entity_match"
                            }
                            results.append(result)
                            seen_ids.add(node.get('id'))
                            if len(results) >= limit:
                                break
                    
                    if len(results) >= limit:
                        break
        else:
            # å¤‡ç”¨æœç´¢
            for symptom in symptoms:
                for node in self.knowledge_graph_data.get("nodes", []):
                    node_label = node.get('label', '').lower()
                    
                    if symptom in node_label and node.get('id') not in seen_ids:
                        # åˆ¤æ–­æ˜¯å¦æ˜¯ç–¾ç—…å®ä½“
                        is_disease = any(disease_keyword in node_label for disease_keyword in ['æ„Ÿå†’', 'å‘çƒ§', 'å’³å—½', 'å¤´ç—›', 'è‚ºç‚', 'èƒƒç‚', 'è‚ç‚', 'é«˜è¡€å‹', 'ç³–å°¿ç—…'])
                        
                        result = {
                            **node,
                            "match_type": "symptom_diagnosis" if is_disease else "symptom_entity",
                            "match_score": 85 if is_disease else 70,
                            "matched_symptoms": [symptom],
                            "search_method": "fallback_symptom_search"
                        }
                        results.append(result)
                        seen_ids.add(node.get('id'))
                        if len(results) >= limit:
                            break
                
                if len(results) >= limit:
                    break
        
        end_time = time.time()
        print(f"[è°ƒè¯•] ç—‡çŠ¶è¯Šæ–­æœç´¢ç»“æœæ•°é‡: {len(results)}, è€—æ—¶ {end_time - start_time:.3f}s")
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.get('match_score', 0), reverse=True)
        return results[:limit]
    
    def _search_concurrent(self, query_intent: Dict[str, Any], limit: int = 8) -> List[Dict[str, Any]]:
        """å¹¶å‘æœç´¢ç›¸å…³å®ä½“"""
        # å¦‚æœæ˜¯ç—‡çŠ¶è¯Šæ–­æŸ¥è¯¢ï¼Œä½¿ç”¨ç—‡çŠ¶æœç´¢
        if query_intent.get('is_symptom_diagnosis') and query_intent.get('symptoms'):
            print(f"[è°ƒè¯•] ç—‡çŠ¶è¯Šæ–­æŸ¥è¯¢: ç—‡çŠ¶={query_intent['symptoms']}")
            return self._search_by_symptoms(query_intent['symptoms'], limit)
        
        # å¦‚æœæ˜¯éç»“æ„åŒ–æŸ¥è¯¢ï¼Œä½¿ç”¨é€šç”¨æœç´¢
        if not query_intent['is_structured_query']:
            return self.search_entities(query_intent['original_query'], limit)
        
        start_time = time.time()
        print(f"[å¹¶å‘æœç´¢] å¼€å§‹å¹¶å‘æœç´¢: ç–¾ç—…={query_intent['disease']}, å…³ç³»={query_intent['relation']}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘æœç´¢
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤å¤šä¸ªæœç´¢ä»»åŠ¡
            future_relation = executor.submit(
                self._search_by_relation, 
                query_intent['disease'], 
                query_intent['relation'], 
                limit
            )
            
            future_entities = executor.submit(
                self.search_entities, 
                query_intent['disease'], 
                limit // 2
            )
            
            future_keywords = executor.submit(
                self.search_entities, 
                query_intent['relation'], 
                limit // 2
            )
            
            # æ”¶é›†ç»“æœ
            results = []
            seen_ids = set()
            
            # å…³ç³»æœç´¢ç»“æœï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            try:
                relation_results = future_relation.result(timeout=2.0)
                for result in relation_results:
                    if result.get('id') not in seen_ids:
                        results.append(result)
                        seen_ids.add(result.get('id'))
            except concurrent.futures.TimeoutError:
                print("[å¹¶å‘æœç´¢] å…³ç³»æœç´¢è¶…æ—¶")
            
            # ç–¾ç—…å®ä½“æœç´¢ç»“æœ
            try:
                entity_results = future_entities.result(timeout=1.0)
                for result in entity_results:
                    if result.get('id') not in seen_ids and len(results) < limit:
                        result['match_score'] = result.get('match_score', 0) * 0.8  # é™ä½åˆ†æ•°
                        results.append(result)
                        seen_ids.add(result.get('id'))
            except concurrent.futures.TimeoutError:
                print("[å¹¶å‘æœç´¢] å®ä½“æœç´¢è¶…æ—¶")
            
            # å…³ç³»å…³é”®è¯æœç´¢ç»“æœ
            try:
                keyword_results = future_keywords.result(timeout=1.0)
                for result in keyword_results:
                    if result.get('id') not in seen_ids and len(results) < limit:
                        result['match_score'] = result.get('match_score', 0) * 0.6  # è¿›ä¸€æ­¥é™ä½åˆ†æ•°
                        results.append(result)
                        seen_ids.add(result.get('id'))
            except concurrent.futures.TimeoutError:
                print("[å¹¶å‘æœç´¢] å…³é”®è¯æœç´¢è¶…æ—¶")
        
        end_time = time.time()
        print(f"[å¹¶å‘æœç´¢] å®Œæˆ, æ‰¾åˆ° {len(results)} ä¸ªç»“æœ, è€—æ—¶ {end_time - start_time:.3f}s")
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.get('match_score', 0), reverse=True)
        return results[:limit] 