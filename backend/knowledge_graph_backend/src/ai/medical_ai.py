"""
åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹æ¨¡å—
"""
from typing import Dict, Any, List, Optional, Tuple
import json
import re
import requests
from collections import defaultdict

from src.config.ai_config import AIConfig, ModelType
from src.utils.graph_cache import graph_cache

class MedicalKnowledgeGraphAI:
    """åŒ»ç–—çŸ¥è¯†å›¾è°±AIåŠ©æ‰‹"""
    
    def __init__(self, knowledge_graph_data: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åŒ»ç–—AIåŠ©æ‰‹
        
        Args:
            knowledge_graph_data: çŸ¥è¯†å›¾è°±æ•°æ®ï¼ŒåŒ…å«nodeså’Œlinks
        """
        if knowledge_graph_data:
            self.knowledge_graph_data = knowledge_graph_data
        else:
            self.knowledge_graph_data = {"nodes": [], "links": []}
            
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
        self.sources_per_page = AIConfig.SOURCES_PAGE_SIZE
        
        # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å­˜ç³»ç»Ÿ
        self._use_cache = True
        
        # åˆå§‹åŒ–LLM
        self.llm = self._init_llm()
    
    def _init_llm(self):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            return self._init_ollama()
        else:
            return self._init_openai()
    
    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaæ¨¡å‹"""
        try:
            # æ£€æŸ¥OllamaæœåŠ¡å¯ç”¨æ€§ï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.get(
                f"{AIConfig.OLLAMA_BASE_URL}/api/tags", 
                timeout=5,
                proxies={'http': '', 'https': ''}
            )
            if response.status_code == 200:
                print(f"[ä¿¡æ¯] OllamaæœåŠ¡å¯ç”¨: {AIConfig.OLLAMA_BASE_URL}")
                return {"type": "ollama", "available": True}
            else:
                print(f"[è­¦å‘Š] OllamaæœåŠ¡ä¸å¯ç”¨ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {"type": "ollama", "available": False}
        except Exception as e:
            print(f"[é”™è¯¯] æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {str(e)}")
            return {"type": "ollama", "available": False}
    
    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIæ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ OpenAIçš„åˆå§‹åŒ–é€»è¾‘
        return {"type": "openai", "available": True}
    
    def _build_entity_index(self) -> Dict[str, Any]:
        """
        æ„å»ºå®ä½“ç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²åºŸå¼ƒï¼Œç°åœ¨ä½¿ç”¨graph_cacheä¸­çš„ä¼˜åŒ–ç´¢å¼•ç³»ç»Ÿ
        """
        print("[è­¦å‘Š] _build_entity_indexæ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨graph_cacheä¼˜åŒ–ç´¢å¼•ç³»ç»Ÿ")
        index = {
            "entities": {},  # å®ä½“IDåˆ°å®ä½“ä¿¡æ¯çš„æ˜ å°„
            "relationships": defaultdict(list),  # å®ä½“é—´å…³ç³»
            "search_index": {}  # æœç´¢ç´¢å¼•ï¼ˆå®ä½“åç§°åˆ°IDçš„æ˜ å°„ï¼‰
        }
        
        # æ„å»ºèŠ‚ç‚¹ç´¢å¼•
        for node in self.knowledge_graph_data.get("nodes", []):
            entity_id = node.get("id")
            entity_label = node.get("label", "")
            
            index["entities"][entity_id] = {
                "id": entity_id,
                "label": entity_label,
                "type": node.get("type", "entity"),
                "connections": node.get("connections", 0)
            }
            
            # æ„å»ºæœç´¢ç´¢å¼•
            if entity_label:
                # æ”¯æŒæ¨¡ç³Šæœç´¢
                label_lower = entity_label.lower()
                index["search_index"][label_lower] = entity_id
                
                # æ·»åŠ éƒ¨åˆ†åŒ¹é…
                words = label_lower.split()
                for word in words:
                    if len(word) > 1:  # é¿å…å•å­—ç¬¦ç´¢å¼•
                        if word not in index["search_index"]:
                            index["search_index"][word] = []
                        if isinstance(index["search_index"][word], list):
                            index["search_index"][word].append(entity_id)
                        else:
                            index["search_index"][word] = [index["search_index"][word], entity_id]
        
        # æ„å»ºå…³ç³»ç´¢å¼•
        for link in self.knowledge_graph_data.get("links", []):
            source = link.get("source")
            target = link.get("target")
            relation = link.get("label", "ç›¸å…³")
            
            if source and target:
                index["relationships"][source].append({
                    "target": target,
                    "relation": relation,
                    "direction": "outgoing"
                })
                index["relationships"][target].append({
                    "target": source,
                    "relation": relation,
                    "direction": "incoming"
                })
        
        return index
    
    def search_entities(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿæœç´¢å®ä½“ - ä½¿ç”¨ä¼˜åŒ–çš„ç´¢å¼•ç®—æ³•
        æ—¶é—´å¤æ‚åº¦ä» O(EÃ—QÃ—W) é™ä½åˆ° O(log N)
        """
        if not query.strip():
            return []
        
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„å¿«é€Ÿæœç´¢
        if self._use_cache and graph_cache.get_cached_graph():
            return graph_cache.search_entities_fast(query, limit)
        
        # å¤‡ç”¨ï¼šåŸå§‹æœç´¢ç®—æ³•
        return self._search_entities_fallback(query, limit)
    
    def _search_entities_fallback(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """å¤‡ç”¨æœç´¢ç®—æ³• - å½“ç¼“å­˜ä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        query_lower = query.lower().strip()
        if not query_lower:
            return []
        
        scored_results = []
        
        # ç®€åŒ–çš„åŒ»ç–—æœ¯è¯­æ˜ å°„
        medical_terms = {
            'æ„Ÿå†’': ['æ„Ÿå†’', 'æ™®é€šæ„Ÿå†’', 'ä¸Šå‘¼å¸é“æ„ŸæŸ“'],
            'å‘çƒ§': ['å‘çƒ­', 'å‘çƒ§', 'ä½“æ¸©å‡é«˜'],
            'å’³å—½': ['å’³å—½', 'å’³ç—°', 'å¹²å’³'],
            'å¤´ç—›': ['å¤´ç—›', 'å¤´ç–¼', 'åå¤´ç—›']
        }
        
        # æ‰©å±•æŸ¥è¯¢è¯
        expanded_queries = [query_lower]
        for term, synonyms in medical_terms.items():
            if term in query_lower:
                expanded_queries.extend(synonyms)
        
        # å¿«é€ŸåŒ¹é…è¯„åˆ†
        for node in self.knowledge_graph_data.get("nodes", []):
            entity_label = node.get("label", "").lower()
            score = 0
            match_type = "none"
            
            # ç²¾ç¡®åŒ¹é…
            if query_lower == entity_label:
                score = 100
                match_type = "exact"
            # åŒ…å«åŒ¹é…
            elif query_lower in entity_label or entity_label in query_lower:
                score = 80
                match_type = "contains"
            # è¯è¯­åŒ¹é…
            else:
                query_words = set(query_lower.split())
                entity_words = set(entity_label.split())
                intersection = query_words & entity_words
                if intersection:
                    score = 60 * len(intersection) / len(query_words)
                    match_type = "partial"
            
            if score > 0:
                result_entity = {
                    **node,
                    "match_type": match_type,
                    "match_score": score
                }
                scored_results.append(result_entity)
        
        # æ’åºå¹¶è¿”å›
        scored_results.sort(key=lambda x: (x["match_score"], x.get("connections", 0)), reverse=True)
        return scored_results[:limit]
    
    def get_entity_context(self, entity_id: str, depth: int = 1) -> Dict[str, Any]:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„å®ä½“ä¿¡æ¯
        if self._use_cache and graph_cache.get_cached_graph():
            cached_graph = graph_cache.get_cached_graph()
            if not cached_graph:
                return {}
                
            entity = None
            
            # æŸ¥æ‰¾å®ä½“
            for node in cached_graph.get('nodes', []):
                if node.get('id') == entity_id:
                    entity = node
                    break
            
            if not entity:
                return {}
            
            context = {
                "entity": entity,
                "relationships": [],
                "neighbors": []
            }
            
            # è·å–å…³ç³»
            for edge in cached_graph.get('edges', []):
                if edge['source'] == entity_id:
                    # æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹
                    for node in cached_graph.get('nodes', []):
                        if node.get('id') == edge['target']:
                            context["relationships"].append({
                                "relation": edge['relation'],
                                "direction": "outgoing",
                                "neighbor": node
                            })
                            context["neighbors"].append(node)
                            break
                elif edge['target'] == entity_id:
                    # æŸ¥æ‰¾æºèŠ‚ç‚¹
                    for node in cached_graph.get('nodes', []):
                        if node.get('id') == edge['source']:
                            context["relationships"].append({
                                "relation": edge['relation'],
                                "direction": "incoming",
                                "neighbor": node
                            })
                            context["neighbors"].append(node)
                            break
            
            return context
        
        # å¤‡ç”¨ï¼šç›´æ¥ä»çŸ¥è¯†å›¾è°±æ•°æ®æŸ¥æ‰¾
        entity = None
        for node in self.knowledge_graph_data.get('nodes', []):
            if node.get('id') == entity_id:
                entity = node
                break
        
        if not entity:
            return {}
        
        context = {
            "entity": entity,
            "relationships": [],
            "neighbors": []
        }
        
        # è·å–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for edge in self.knowledge_graph_data.get('edges', []):
            if edge['source'] == entity_id or edge['target'] == entity_id:
                neighbor_id = edge['target'] if edge['source'] == entity_id else edge['source']
                direction = "outgoing" if edge['source'] == entity_id else "incoming"
                
                # æŸ¥æ‰¾é‚»å±…èŠ‚ç‚¹
                for node in self.knowledge_graph_data.get('nodes', []):
                    if node.get('id') == neighbor_id:
                        context["relationships"].append({
                            "relation": edge['relation'],
                            "direction": direction,
                            "neighbor": node
                        })
                        context["neighbors"].append(node)
                        break
        
        return context
    
    def _call_ollama(self, prompt: str, context: str = "") -> str:
        """è°ƒç”¨Ollamaæ¨¡å‹"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            if context:
                full_prompt += f"çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼š\n{context}\n\n"
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\nè¯·åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            
            # è°ƒç”¨Ollama APIï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False
                },
                timeout=30,
                proxies={'http': '', 'https': ''}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def ask(self, question: str) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›å›ç­”"""
        if not question.strip():
            return {
                "answer": "è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜ã€‚",
                "related_entities": [],
                "suggested_focus": None,
                "sources": [],
                "medical_disclaimer": True
            }
        
        # æœç´¢ç›¸å…³å®ä½“
        related_entities = self.search_entities(question, limit=8)
        
        # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        if related_entities:
            context_info.append("=== çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ ===")
            for entity in related_entities:
                entity_context = self.get_entity_context(entity["id"])
                context_info.append(f"\nå®ä½“: {entity['label']}")
                context_info.append(f"ID: {entity['id']}")
                context_info.append(f"ç±»å‹: {entity.get('type', 'æœªçŸ¥')}")
                context_info.append(f"è¿æ¥æ•°: {entity.get('connections', 0)}")
                
                # æ·»åŠ å…³ç³»ä¿¡æ¯
                relationships = entity_context.get("relationships", [])
                if relationships:
                    context_info.append("å…³ç³»:")
                    for rel in relationships[:5]:  # é™åˆ¶å…³ç³»æ•°é‡
                        direction_symbol = "â†’" if rel['direction'] == 'outgoing' else "â†"
                        context_info.append(f"  {direction_symbol} {rel['relation']}: {rel['neighbor']['label']} (ID: {rel['neighbor']['id']})")
                else:
                    context_info.append("å…³ç³»: æ— ç›´æ¥å…³ç³»")
                    
            context_info.append("\n=== ä¸Šä¸‹æ–‡ç»“æŸ ===")
        
        context_text = "\n".join(context_info) if context_info else "æœªæ‰¾åˆ°ç›¸å…³å®ä½“"
        
        # è°ƒç”¨AIæ¨¡å‹
        if AIConfig.MODEL_TYPE == ModelType.OLLAMA:
            answer = self._call_ollama_strict(question, context_text, related_entities)
        else:
            answer = "OpenAIæ¨¡å‹æš‚æœªå®ç°"
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å®ä½“ï¼Œæä¾›æ ‡å‡†å›ç­”
        if not related_entities:
            answer = self._generate_no_knowledge_response(question)
        
        # éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨
        validated_answer = self._validate_entity_references(answer, related_entities)
        
        # å»ºè®®èšç„¦çš„èŠ‚ç‚¹ï¼ˆé€‰æ‹©æœ€ç›¸å…³çš„å®ä½“ï¼‰
        suggested_focus = related_entities[0]["id"] if related_entities else None
        
        # ä¿å­˜åˆ°èŠå¤©å†å²
        self.chat_history.append({
            "question": question,
            "answer": validated_answer,
            "related_entities": related_entities,
            "context_used": context_text,
            "timestamp": self._get_timestamp()
        })
        
        # æ›´æ–°å½“å‰æ¥æº
        self.current_sources = related_entities
        self.current_page = 0
        
        return {
            "answer": validated_answer,
            "related_entities": related_entities,
            "suggested_focus": suggested_focus,
            "sources": self._get_paginated_sources(),
            "medical_disclaimer": True,
            "context_used": context_text,
            "knowledge_graph_coverage": len(related_entities) > 0
        }

    def _call_ollama_strict(self, prompt: str, context: str = "", entities: Optional[List[Dict]] = None) -> str:
        """ä¸¥æ ¼è°ƒç”¨Ollamaæ¨¡å‹ï¼Œå¼ºåŒ–çŸ¥è¯†å›¾è°±çº¦æŸ"""
        if not self.llm.get("available", False):
            return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        try:
            # æ„å»ºä¸¥æ ¼çš„æç¤ºè¯
            full_prompt = f"{AIConfig.MEDICAL_AI_PROMPT}\n\n"
            
            if context and entities:
                full_prompt += f"{context}\n\n"
                full_prompt += "ã€å¯å¼•ç”¨çš„å®ä½“IDåˆ—è¡¨ã€‘ï¼š\n"
                for entity in entities:
                    full_prompt += f"- {entity['label']} (ID: {entity['id']})\n"
                full_prompt += "\n"
            else:
                full_prompt += "çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼šæ— ç›¸å…³å®ä½“\n\n"
            
            full_prompt += f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\n\n"
            full_prompt += "è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°çº¦æŸå›ç­”ï¼Œåªèƒ½ä½¿ç”¨ä¸Šè¿°ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å’Œå®ä½“IDã€‚å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            # è°ƒç”¨Ollama APIï¼ˆç¦ç”¨ä»£ç†ï¼‰
            response = requests.post(
                f"{AIConfig.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": AIConfig.OLLAMA_MODEL_NAME,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # é™ä½æ¸©åº¦ï¼Œå‡å°‘åˆ›é€ æ€§
                        "top_p": 0.8,
                        "top_k": 10
                    }
                },
                timeout=30,
                proxies={'http': '', 'https': ''}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚")
            else:
                return f"AIæœåŠ¡é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{response.status_code}ï¼‰"
                
        except Exception as e:
            print(f"[é”™è¯¯] è°ƒç”¨Ollamaå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"

    def _validate_entity_references(self, answer: str, valid_entities: List[Dict]) -> str:
        """éªŒè¯AIå›ç­”ä¸­çš„å®ä½“å¼•ç”¨ï¼Œç§»é™¤æ— æ•ˆçš„å®ä½“ID"""
        if not valid_entities:
            return answer
        
        # æå–æœ‰æ•ˆçš„å®ä½“ID
        valid_ids = set(entity['id'] for entity in valid_entities)
        valid_labels = set(entity['label'] for entity in valid_entities)
        
        # æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«æ— æ•ˆçš„å®ä½“IDæ¨¡å¼
        import re
        
        # æŸ¥æ‰¾å¯èƒ½çš„å®ä½“IDæ¨¡å¼ï¼ˆå¦‚ D123, F456, B001 ç­‰ï¼‰
        id_pattern = r'\b[A-Z]\d+\b'
        found_ids = re.findall(id_pattern, answer)
        
        # ç§»é™¤æ— æ•ˆçš„å®ä½“ID
        validated_answer = answer
        for found_id in found_ids:
            if found_id not in valid_ids:
                # ç§»é™¤æ— æ•ˆçš„å®ä½“IDå¼•ç”¨
                validated_answer = re.sub(rf'\([^)]*{re.escape(found_id)}[^)]*\)', '', validated_answer)
                validated_answer = re.sub(rf'\b{re.escape(found_id)}\b', '', validated_answer)
        
        # å¦‚æœå›ç­”ä¸­æ²¡æœ‰å¼•ç”¨ä»»ä½•æœ‰æ•ˆå®ä½“ä¸”åŸæœ¬æœ‰ç›¸å…³å®ä½“ï¼Œæ·»åŠ æç¤º
        has_valid_reference = any(entity['label'] in answer or entity['id'] in answer for entity in valid_entities)
        if valid_entities and not has_valid_reference and 'çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°' not in answer:
            validated_answer += f"\n\nğŸ’¡ ç›¸å…³å®ä½“ï¼šåŸºäºæ‚¨çš„é—®é¢˜ï¼Œåœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°äº†ç›¸å…³å®ä½“ï¼š{', '.join([e['label'] for e in valid_entities[:3]])}ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ã€‚"
        
        return validated_answer
    
    def _extract_entity_references(self, text: str, entities: List[Dict]) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å¼•ç”¨"""
        referenced = []
        for entity in entities:
            if entity["label"].lower() in text.lower():
                referenced.append(entity["id"])
        return referenced
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _get_paginated_sources(self) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„æ¥æºä¿¡æ¯"""
        start_idx = self.current_page * self.sources_per_page
        end_idx = start_idx + self.sources_per_page
        
        page_sources = self.current_sources[start_idx:end_idx]
        total_pages = (len(self.current_sources) + self.sources_per_page - 1) // self.sources_per_page
        
        return {
            "sources": page_sources,
            "current_page": self.current_page + 1,
            "total_pages": total_pages,
            "total_sources": len(self.current_sources),
            "has_next": end_idx < len(self.current_sources),
            "has_prev": self.current_page > 0
        }
    
    def next_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸‹ä¸€é¡µæ¥æº"""
        if (self.current_page + 1) * self.sources_per_page < len(self.current_sources):
            self.current_page += 1
        return self._get_paginated_sources()
    
    def prev_page(self) -> Dict[str, Any]:
        """ç¿»åˆ°ä¸Šä¸€é¡µæ¥æº"""
        if self.current_page > 0:
            self.current_page -= 1
        return self._get_paginated_sources()
    
    def get_chat_history(self, page: int = 1, page_size: Optional[int] = None) -> Dict[str, Any]:
        """è·å–åˆ†é¡µçš„èŠå¤©å†å²"""
        # ç¡®ä¿page_sizeæ˜¯intç±»å‹
        actual_page_size: int = page_size if page_size is not None else AIConfig.CHAT_PAGE_SIZE
        
        start_idx = (page - 1) * actual_page_size
        end_idx = start_idx + actual_page_size
        
        page_history = self.chat_history[start_idx:end_idx]
        total_pages = (len(self.chat_history) + actual_page_size - 1) // actual_page_size
        
        return {
            "history": page_history,
            "current_page": page,
            "total_pages": total_pages,
            "total_chats": len(self.chat_history),
            "has_next": end_idx < len(self.chat_history),
            "has_prev": page > 1
        }
    
    def clear_history(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_history = []
        self.current_sources = []
        self.current_page = 0
    
    def update_knowledge_graph(self, graph_data: Dict[str, Any]):
        """æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®"""
        self.knowledge_graph_data = graph_data
        # ä¸å†ä½¿ç”¨entity_indexï¼Œæ”¹ç”¨ç¼“å­˜ç³»ç»Ÿ
        print(f"[ä¿¡æ¯] çŸ¥è¯†å›¾è°±å·²æ›´æ–°ï¼ŒåŒ…å« {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹") 

    def update_knowledge_graph_from_file(self, csv_file_path: str, force_reload: bool = False):
        """ä»CSVæ–‡ä»¶æ›´æ–°çŸ¥è¯†å›¾è°±ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å­˜åŠ è½½
            graph_data = graph_cache.load_graph(csv_file_path, force_reload)
            self.knowledge_graph_data = graph_data
            print(f"[ä¿¡æ¯] çŸ¥è¯†å›¾è°±å·²æ›´æ–°ï¼ŒåŒ…å« {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            print(f"[é”™è¯¯] æ›´æ–°çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            self.knowledge_graph_data = {"nodes": [], "links": []}

    def _generate_no_knowledge_response(self, question: str) -> str:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶çš„æ ‡å‡†å›ç­”"""
        return f"""å¾ˆæŠ±æ­‰ï¼Œåœ¨å½“å‰åŒ»ç–—çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°å…³äº"{question}"çš„ç›¸å…³ä¿¡æ¯ã€‚

å½“å‰çŸ¥è¯†å›¾è°±ä¸»è¦åŒ…å«ä»¥ä¸‹ç±»å‹çš„åŒ»ç–—ä¿¡æ¯ï¼š
- ç–¾ç—…ç›¸å…³ä¿¡æ¯
- ç—‡çŠ¶æè¿°
- æ²»ç–—æ–¹æ³•
- è¯ç‰©ä¿¡æ¯
- æ£€æŸ¥é¡¹ç›®
- èº«ä½“éƒ¨ä½

å»ºè®®æ‚¨ï¼š
1. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„åŒ»ç–—æœ¯è¯­é‡æ–°æé—®
2. å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®çš„åŒ»ç–—å»ºè®®

âš ï¸ åŒ»ç–—å…è´£å£°æ˜ï¼šæœ¬ç³»ç»Ÿä»…æä¾›åŸºäºçŸ¥è¯†å›¾è°±çš„ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰å¥åº·é—®é¢˜ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚""" 